{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Sign Classification with Tensorflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Load Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define variables\n",
    "setnames = []\n",
    "data     = dict()\n",
    "X        = dict()\n",
    "y        = dict()\n",
    "\n",
    "path     = \"/home/tiwi/Documents/mldata/\"\n",
    "zipfiles = \"traffic-signs-data.zip\"\n",
    "file_concat    = lambda path, zipfiles, filename: path + zipfiles.rpartition('.')[0] + '/' + filename\n",
    "file_body_of   = lambda filename: os.path.split(filename)[1].rpartition('.')[0] \n",
    "\n",
    "print(\"Filenames in data container: \\n\")\n",
    "for filename in ZipFile(path+zipfiles).namelist():\n",
    "    print(filename)\n",
    "    # Create setnames from filenames: e.g. \"train\", \"test\", ...\n",
    "    setnames.append(file_body_of(filename))\n",
    "    # Get full path + file for each pickle file\n",
    "    with open(file_concat(path, zipfiles, filename), mode='rb') as f:\n",
    "        data[setnames[-1]] = pickle.load(f)\n",
    "        \n",
    "for setname in setnames:\n",
    "    X[setname], y[setname] = data[setname]['features'], data[setname]['labels']\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1: Print Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classes = pd.read_csv(\"../CarND-Traffic-Sign-Classifier-Project/signnames.csv\")\n",
    "nb_classes = len(set(y[\"train\"]))\n",
    "\n",
    "print(\"Number of training examples:\\t\", X[\"train\"].shape[0])\n",
    "print(\"Number of testing examples:\\t\",  X[\"test\"].shape[0])\n",
    "print(\"Image data shape:\\t\\t\",          X[\"train\"].shape[1:])\n",
    "print(\"Number of classes:\\t\\t\",         nb_classes)\n",
    "\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_traffic_signs(X, rnd_seed=1):\n",
    "    \n",
    "    # Define square number of example images\n",
    "    n = np.int(np.floor(np.sqrt(nb_classes)))\n",
    "    nb_plots = n**2\n",
    "    np.random.seed(rnd_seed)\n",
    "    random_images = [X[\"train\"][i,::] for i in np.random.randint(0, X[\"train\"].shape[0], nb_plots)]\n",
    "\n",
    "    # Setup a grid [n x n] grid of subplots\n",
    "    fig, axes = plt.subplots(n, n, figsize=(12, 12), subplot_kw={'xticks': [], 'yticks': []})\n",
    "    fig.subplots_adjust(hspace=0.0, wspace=0.0)\n",
    "\n",
    "    for img, ax, img_class in zip(random_images, axes.flat, classes[\"SignName\"][0:nb_plots]):\n",
    "        if img.shape[-1] < 3:\n",
    "            ax.imshow(img.squeeze())\n",
    "        else:\n",
    "            ax.imshow(img)\n",
    "\n",
    "        \n",
    "    plt.show()\n",
    "plot_traffic_signs(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Design and Test a Model Architecture\n",
    "The basic idea is to implement a simple CNN archictecture that gets a reasonable accurancy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing 1: Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize_brightness(features):\n",
    "    equ = dict()\n",
    "    for key in features: \n",
    "        sx, sy = features[key].shape[1], features[key].shape[2] \n",
    "        equ[key] = np.zeros((features[key].shape[0:3]))\n",
    "        for idx, img in enumerate(features[key]):\n",
    "            tmp_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "            tmp_img = cv2.equalizeHist(tmp_img)\n",
    "            equ[key][idx,:,:] = tmp_img\n",
    "        equ[key] = equ[key].reshape(-1, sx, sy, 1)\n",
    "        print(equ[key].shape)\n",
    "        \n",
    "    return equ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Before Normalization\")\n",
    "print(\"Shape:\\t\", X[\"train\"].shape)\n",
    "print(\"Min:\\t\", X[\"train\"].min())\n",
    "print(\"Max:\\t\", X[\"train\"].max())\n",
    "print(\"Mean:\\t\", np.mean(X[\"train\"]))\n",
    "\n",
    "print(\"Shape y:\", y[\"train\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer, OneHotEncoder\n",
    "\n",
    "def normalize(features, method='standard'):\n",
    "    X = features.copy()\n",
    "    if method=='standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif method=='norm':\n",
    "        scaler = Normalizer()\n",
    "    elif method=='minmax':\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        \n",
    "    for key in X:\n",
    "        shape      = X[key].shape\n",
    "        image_size = shape[1] * shape[2] * shape[3]    \n",
    "        X[key]     = X[key].reshape(-1, image_size)\n",
    "        X[key]     = scaler.fit_transform(np.float32(X[key])).reshape(-1, shape[1], shape[2], shape[3])\n",
    "    return X\n",
    "\n",
    "def encode(labels):\n",
    "    y = labels.copy()\n",
    "    nb_classes = len(set(y[\"train\"]))\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "    for key in y:\n",
    "        y[key] = encoder.fit_transform(y[key].reshape(-1, 1))\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "def get_batches(features, labels, batch_size):\n",
    "    nb_samples = len(features)\n",
    "    split_at = np.arange(batch_size, nb_samples, batch_size)\n",
    "    \n",
    "    features, labels = shuffle(features, labels)\n",
    "    \n",
    "    feature_batches = np.split(features, split_at)\n",
    "    label_batches   = np.split(labels, split_at)\n",
    "    \n",
    "    return feature_batches, label_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = normalize_brightness(X)\n",
    "X = normalize(X, method='minmax')\n",
    "Y = encode(y)\n",
    "\n",
    "print(\"\\nAfter Normalization\")\n",
    "print(\"Shape:\\t\", X[\"train\"].shape)\n",
    "print(\"Min:\\t\", X[\"train\"].min())\n",
    "print(\"Max:\\t\", X[\"train\"].max())\n",
    "print(\"Mean:\\t\", np.mean(X[\"train\"]))\n",
    "\n",
    "print(\"Shape y:\", y[\"train\"].shape)\n",
    "\n",
    "print(\"Shape Y:\", Y[\"train\"].shape)\n",
    "print(\"Type Y:\", type(Y[\"train\"]))\n",
    "\n",
    "assert type(Y['train']) == type(X['train'])\n",
    "assert len(Y['train']) == len(y['train'])\n",
    "\n",
    "plot_traffic_signs(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def conv_relu_layer(input, kernel_shape, bias_shape):\n",
    "    # Create variable named \"weights\".\n",
    "    weights = tf.get_variable(\"weights\", kernel_shape, initializer=tf.random_normal_initializer())\n",
    "    # Create variable named \"biases\".\n",
    "    biases = tf.get_variable(\"biases\", bias_shape, initializer=tf.constant_initializer(0.0))\n",
    "    conv_ = tf.nn.conv2d(input, weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return tf.nn.relu(conv_ + biases)\n",
    "\n",
    "def dense_layer(layer, output_shape, bias_shape, activation='relu'):\n",
    "    \n",
    "    weights_shape = [layer.shape[1], output_shape]\n",
    "    dense1_W = tf.get_variable(\"weights\", weights_shape, initializer=tf.random_normal_initializer())\n",
    "    dense1_b = tf.get_variable(\"biases\", bias_shape, initializer=tf.constant_initializer(0.0))\n",
    "    dense1_   = tf.matmul(layer, dense1_W) + dense1_b \n",
    "    \n",
    "    if activation=='relu':\n",
    "        return tf.nn.relu(dense1_)\n",
    "    elif activation=='softmax':\n",
    "        return tf.nn.softmax(dense1_)\n",
    "    elif activation=='none':\n",
    "        return dense1_\n",
    "\n",
    "def create_model(input_images, params, nb_classes, scope_reuse=True):\n",
    "    \n",
    "    nb_layers = len(params[\"nb_kernels\"])\n",
    "    \n",
    "    kx   = params[\"kernel_size\"][0][0]\n",
    "    ky   = params[\"kernel_size\"][0][1]\n",
    "    nb_0 = input_images.shape[3]\n",
    "    nb_1 = params[\"nb_kernels\"][0]\n",
    "    \n",
    "    with tf.variable_scope(\"conv_layer_0\", reuse=scope_reuse):\n",
    "        conv = conv_relu_layer(input_images, [kx, ky, nb_0, nb_1], [nb_1])\n",
    "    \n",
    "    for n in range(1, nb_layers):\n",
    "        nb_0 = nb_1\n",
    "        kx   = params[\"kernel_size\"][n][0]\n",
    "        ky   = params[\"kernel_size\"][n][1]\n",
    "        nb_1 = params[\"nb_kernels\"][n]\n",
    "    \n",
    "        with tf.variable_scope(\"conv_layer_\"+str(n), reuse=scope_reuse):\n",
    "            # Variables created here will be named \"conv1/weights\", \"conv1/biases\".\n",
    "            conv = conv_relu_layer(conv, [kx, ky, nb_0, nb_1], [nb_1])\n",
    "    \n",
    "    conv1 = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    conv2 = flatten(conv1)\n",
    "    with tf.variable_scope(\"dense_layer_0\", reuse=scope_reuse):\n",
    "        dense0 = dense_layer(conv2, 100, 100, activation='relu')\n",
    "    \n",
    "    keep_prob = tf.constant(0.25, dtype='float32')\n",
    "    dense0_drop = tf.nn.dropout(dense0, keep_prob=keep_prob)\n",
    "    with tf.variable_scope(\"dense_layer_1\", reuse=scope_reuse):\n",
    "        logits = dense_layer(dense0_drop, nb_classes, nb_classes, activation='none')\n",
    "            \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "conv_params = dict({'kernel_size': [], 'nb_kernels':[]})\n",
    "conv_params['kernel_size'] = [[3, 1],\n",
    "                              [1, 3],\n",
    "                              [3, 3],\n",
    "                              [5, 5]]\n",
    "conv_params['nb_kernels'] = [6,\n",
    "                             6,\n",
    "                             12,\n",
    "                             32]    \n",
    "\n",
    "x         = tf.placeholder(tf.float32, shape=(None, 32, 32, 1))\n",
    "one_hot_y = tf.placeholder(tf.float32, shape=(None, nb_classes))\n",
    "\n",
    "logits = create_model(x, params=conv_params, nb_classes=nb_classes, scope_reuse=False)\n",
    "\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=one_hot_y)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.02).minimize(loss)\n",
    "#optimizer = tf.train.AdadeltaOptimizer(learning_rate=1.0, rho=0.95, epsilon=1e-08).minimize(loss)\n",
    "\n",
    "#keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "def evaluate(X_batches, Y_batches, accuracy_operation):\n",
    "    \n",
    "    total_accuracy   = 0\n",
    "    nb_total_samples = 0\n",
    "    sess = tf.get_default_session()\n",
    "    \n",
    "    for X_batch, Y_batch in zip(X_batches, Y_batches):\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: X_batch, one_hot_y: Y_batch})\n",
    "        \n",
    "        nb_samples        = len(Y_batch)\n",
    "        total_accuracy   += (accuracy * nb_samples)\n",
    "        nb_total_samples += nb_samples\n",
    "\n",
    "    return total_accuracy / nb_total_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nb_epochs = 1\n",
    "nb_samples = 3000\n",
    "batch_size = 150\n",
    "losses = []\n",
    "validation_accuracy = []\n",
    "X_batches, Y_batches = get_batches(X['train'][0:nb_samples,::],\n",
    "                                   Y['train'][0:nb_samples,::],\n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "\n",
    "for X_batch, Y_batch in zip(X_batches, Y_batches):\n",
    "    assert(X_batch.shape[0]==batch_size)\n",
    "    assert(Y_batch.shape[0]==batch_size)\n",
    "    \n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "checkpoint_file = \"./checkpoints/model.ckpt\"\n",
    "checkpoint_path = \"\"\n",
    "\n",
    "with tf.Session() as session:\n",
    "    # Restore variables from disk.\n",
    "    if os.path.isfile(checkpoint_file + \".meta\"):\n",
    "        saver.restore(session, tf.train.latest_checkpoint('./checkpoints/'))\n",
    "        print(\"Model restored.\")\n",
    "    else:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "    for epoch in range(nb_epochs):\n",
    "        print(\"EPOCH {} ...\".format(epoch+1))\n",
    "        X_batches, Y_batches = shuffle(X_batches, Y_batches)\n",
    "        for X_batch, Y_batch in zip(X_batches, Y_batches):\n",
    "            assert(X_batch.shape[0]==batch_size)\n",
    "            assert(Y_batch.shape[0]==batch_size)\n",
    "            X_batch, Y_batch = shuffle(X_batch, Y_batch)\n",
    "            _,l = session.run([optimizer, loss], feed_dict={x:X_batch, one_hot_y:Y_batch})\n",
    "            losses.append(l)\n",
    "            print(\"Loss = {:.3f}\".format(l))\n",
    "\n",
    "        acc = evaluate(X_batches[0:2], Y_batches[0:2], accuracy_operation)\n",
    "        validation_accuracy.append(acc)\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(acc))\n",
    "        # Save the variables to disk.\n",
    "        checkpoint_path = saver.save(session, checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(checkpoint_file)\n",
    "print(checkpoint_path)\n",
    "\n",
    "print(os.path.exists(checkpoint_file + '*'))\n",
    "os.path.exists('./checkpoint')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "XX = dict()\n",
    "for setname in setnames:\n",
    "    XX[setname] = data[setname]['features']\n",
    "   \n",
    "\n",
    "res = normalize_brightness(XX)\n",
    "\n",
    "print(res[\"train\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDos\n",
    "* add brightness normalization\n",
    "* augement dataset to get balanced labels distribution\n",
    "* run full dataset on AWS\n",
    "* start with tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(X[\"train\"].shape[0:3].append(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
