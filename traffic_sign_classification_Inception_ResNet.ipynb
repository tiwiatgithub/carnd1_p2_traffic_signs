{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Sign Classification with Tensorflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Load Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define variables\n",
    "setnames = []\n",
    "data     = dict()\n",
    "X        = dict()\n",
    "y        = dict()\n",
    "\n",
    "path     = \"/home/tiwi/Documents/mldata/\"\n",
    "zipfiles = \"traffic-signs-data.zip\"\n",
    "file_concat    = lambda path, zipfiles, filename: path + zipfiles.rpartition('.')[0] + '/' + filename\n",
    "file_body_of   = lambda filename: os.path.split(filename)[1].rpartition('.')[0] \n",
    "\n",
    "print(\"Filenames in data container: \\n\")\n",
    "for filename in ZipFile(path+zipfiles).namelist():\n",
    "    print(filename)\n",
    "    # Create setnames from filenames: e.g. \"train\", \"test\", ...\n",
    "    setnames.append(file_body_of(filename))\n",
    "    # Get full path + file for each pickle file\n",
    "    with open(file_concat(path, zipfiles, filename), mode='rb') as f:\n",
    "        data[setnames[-1]] = pickle.load(f)\n",
    "        \n",
    "for setname in setnames:\n",
    "    X[setname], y[setname] = data[setname]['features'], data[setname]['labels']\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1: Print Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classes = pd.read_csv(\"../CarND-Traffic-Sign-Classifier-Project/signnames.csv\")\n",
    "nb_classes = len(set(y[\"train\"]))\n",
    "\n",
    "print(\"Number of training examples:\\t\", X[\"train\"].shape[0])\n",
    "print(\"Number of testing examples:\\t\",  X[\"test\"].shape[0])\n",
    "print(\"Image data shape:\\t\\t\",          X[\"train\"].shape[1:])\n",
    "print(\"Number of classes:\\t\\t\",         nb_classes)\n",
    "\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_traffic_signs(X, rnd_seed=1):\n",
    "    \n",
    "    # Define square number of example images\n",
    "    n = np.int(np.floor(np.sqrt(nb_classes)))\n",
    "    nb_plots = n**2\n",
    "    np.random.seed(rnd_seed)\n",
    "    random_images = [X[\"train\"][i,::] for i in np.random.randint(0, X[\"train\"].shape[0], nb_plots)]\n",
    "\n",
    "    # Setup a grid [n x n] grid of subplots\n",
    "    fig, axes = plt.subplots(n, n, figsize=(12, 12), subplot_kw={'xticks': [], 'yticks': []})\n",
    "    fig.subplots_adjust(hspace=0.0, wspace=0.0)\n",
    "\n",
    "    for img, ax, img_class in zip(random_images, axes.flat, classes[\"SignName\"][0:nb_plots]):\n",
    "        if img.shape[-1] < 3:\n",
    "            ax.imshow(img.squeeze())\n",
    "        else:\n",
    "            ax.imshow(img)\n",
    "\n",
    "        \n",
    "    plt.show()\n",
    "plot_traffic_signs(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Design and Test a Model Architecture\n",
    "The basic idea is to implement a simple CNN archictecture that gets a reasonable accurancy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing 1: Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize_brightness(features):\n",
    "    equ = dict()\n",
    "    for key in features: \n",
    "        sx, sy = features[key].shape[1], features[key].shape[2] \n",
    "        equ[key] = np.zeros((features[key].shape[0:3]))\n",
    "        for idx, img in enumerate(features[key]):\n",
    "            tmp_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "            tmp_img = cv2.equalizeHist(tmp_img)\n",
    "            equ[key][idx,:,:] = tmp_img\n",
    "        equ[key] = equ[key].reshape(-1, sx, sy, 1)\n",
    "        print(equ[key].shape)\n",
    "        \n",
    "    return equ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Before Normalization\")\n",
    "print(\"Shape:\\t\", X[\"train\"].shape)\n",
    "print(\"Min:\\t\", X[\"train\"].min())\n",
    "print(\"Max:\\t\", X[\"train\"].max())\n",
    "print(\"Mean:\\t\", np.mean(X[\"train\"]))\n",
    "\n",
    "print(\"Shape y:\", y[\"train\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer, OneHotEncoder\n",
    "\n",
    "def normalize(features, method='standard'):\n",
    "    X = features.copy()\n",
    "    if method=='standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif method=='norm':\n",
    "        scaler = Normalizer()\n",
    "    elif method=='minmax':\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        \n",
    "    for key in X:\n",
    "        shape      = X[key].shape\n",
    "        image_size = shape[1] * shape[2] * shape[3]    \n",
    "        X[key]     = X[key].reshape(-1, image_size)\n",
    "        X[key]     = scaler.fit_transform(np.float32(X[key])).reshape(-1, shape[1], shape[2], shape[3])\n",
    "    return X\n",
    "\n",
    "def encode(labels):\n",
    "    y = labels.copy()\n",
    "    nb_classes = len(set(y[\"train\"]))\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "    for key in y:\n",
    "        y[key] = encoder.fit_transform(y[key].reshape(-1, 1))\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "def get_batches(features, labels, batch_size):\n",
    "    nb_samples = len(features)\n",
    "    split_at = np.arange(batch_size, nb_samples, batch_size)\n",
    "    \n",
    "    features, labels = shuffle(features, labels)\n",
    "    \n",
    "    feature_batches = np.split(features, split_at)\n",
    "    label_batches   = np.split(labels, split_at)\n",
    "    \n",
    "    return feature_batches, label_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = normalize_brightness(X)\n",
    "X = normalize(X, method='minmax')\n",
    "Y = encode(y)\n",
    "\n",
    "print(\"\\nAfter Normalization\")\n",
    "print(\"Shape:\\t\", X[\"train\"].shape)\n",
    "print(\"Min:\\t\", X[\"train\"].min())\n",
    "print(\"Max:\\t\", X[\"train\"].max())\n",
    "print(\"Mean:\\t\", np.mean(X[\"train\"]))\n",
    "\n",
    "print(\"Shape y:\", y[\"train\"].shape)\n",
    "\n",
    "print(\"Shape Y:\", Y[\"train\"].shape)\n",
    "print(\"Type Y:\", type(Y[\"train\"]))\n",
    "\n",
    "assert type(Y['train']) == type(X['train'])\n",
    "assert len(Y['train']) == len(y['train'])\n",
    "\n",
    "plot_traffic_signs(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dense(x, nb_nodes, scope):\n",
    "    return tf.contrib.layers.fully_connected(x, nb_nodes, activation_fn=None, scope=scope)\n",
    "\n",
    "def dense_bn(x, nb_nodes, phase, scope):\n",
    "    with tf.variable_scope(scope):\n",
    "        h1 = tf.contrib.layers.fully_connected(x, nb_nodes, \n",
    "                                               activation_fn=None,\n",
    "                                               scope='dense')\n",
    "        h2 = tf.contrib.layers.batch_norm(h1, \n",
    "                                          center=True, scale=True, \n",
    "                                          is_training=phase,\n",
    "                                          scope='bn')\n",
    "        return tf.nn.relu(h2, 'relu')\n",
    "    \n",
    "def conv_bn(x, nb_filters, kernel_size, phase, activation='relu', stride=1, padding='SAME', scope='conv_nb'):\n",
    "    with tf.variable_scope(scope):\n",
    "        h1 = tf.contrib.layers.conv2d(x, nb_filters, kernel_size,\n",
    "                                      stride=stride, padding=padding,\n",
    "                                      activation_fn=None, scope='conv') \n",
    "        h2 = tf.contrib.layers.batch_norm(h1, \n",
    "                                          center=True, scale=True, \n",
    "                                          is_training=phase,\n",
    "                                          scope='bn')\n",
    "        if activation == None:\n",
    "            return h2\n",
    "        elif activation == 'relu':\n",
    "            return tf.nn.relu(h2, 'relu')\n",
    "    \n",
    "def residuum_stack(x, nb_filters, kernel_size, phase, scope='res_stack'):\n",
    "    \n",
    "    input_size  = x.get_shape()[3]\n",
    "    output_size = nb_filters\n",
    "    print(\"Sizes: \", input_size, output_size)\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "        with tf.variable_scope('A'):\n",
    "            h = conv_bn(x, nb_filters, kernel_size, phase, activation='relu', scope='conv_bn')\n",
    "        with tf.variable_scope('B'):\n",
    "            h = conv_bn(h, nb_filters, kernel_size, phase, activation='relu', scope='conv_bn')\n",
    "\n",
    "        if input_size == output_size:\n",
    "            shortcut = x\n",
    "            print(\"in==out:\", shortcut.get_shape()[3], h.get_shape()[3])\n",
    "            return tf.nn.relu(h + shortcut, 'relu')\n",
    "\n",
    "        else:\n",
    "            shortcut = tf.contrib.layers.conv2d(x,\n",
    "                                            output_size,\n",
    "                                            kernel_size=[1,1],\n",
    "                                            activation_fn=None,\n",
    "                                            scope='conv_1x1')\n",
    "            if input_size > output_size:\n",
    "                print(\"in>out:\", x.get_shape()[3], h.get_shape()[3])\n",
    "            elif input_size < output_size:\n",
    "                print(\"in<out:\", x.get_shape()[3], h.get_shape()[3])\n",
    "\n",
    "            return tf.nn.relu(h + shortcut, 'relu')\n",
    "        \n",
    "def inception_A(x, phase, scope='block_A'):\n",
    "    sc = lambda name, n: name + '_' + str(n) + 'x' + str(n)\n",
    "    nb_filters_1 = 64\n",
    "    nb_filters_2 = 96\n",
    "    with tf.variable_scope(scope):\n",
    "        with tf.variable_scope('branch_0'):            \n",
    "            shortcut = conv_bn(x, nb_filters_2, [1,1], phase, scope='shortcut')\n",
    "\n",
    "        with tf.variable_scope('branch_1'):\n",
    "            h1 = conv_bn(x, nb_filters_2, [1,1], phase, scope='conv_1_1x1')\n",
    "            \n",
    "        with tf.variable_scope('branch_2'):\n",
    "            h2a = conv_bn(x, nb_filters_1, [1,1], phase, scope='conv_2a_1x1')\n",
    "            h2 = conv_bn(h2a, nb_filters_2, [3,3], phase, scope='conv_2b_3x3')\n",
    "            \n",
    "        with tf.variable_scope('branch_3'):\n",
    "            h3a = conv_bn(x, nb_filters_1, [1,1], phase, scope='conv_3a_1x1') \n",
    "            h3b = conv_bn(h3a, nb_filters_2, [3,3], phase, scope='conv_3b_3x3') \n",
    "            h3  = conv_bn(h3b, nb_filters_2, [3,3], phase, scope='conv_3c_3x3')\n",
    "        return tf.concat([shortcut, h1, h2, h3], 3, 'concat')\n",
    "            \n",
    "def inception_B(x, phase, scope='block_B'):\n",
    "\n",
    "    nb_filters_1 = 16  #128\n",
    "    nb_filters_2 = 32  #192\n",
    "    nb_filters_3 = 48 #224\n",
    "    nb_filters_4 = 64 #256\n",
    "    nb_filters_5 = 72 #384\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "        with tf.variable_scope('branch_0'):            \n",
    "            shortcut = conv_bn(x, nb_filters_5, [1,1], phase, scope='shortcut')\n",
    "\n",
    "        with tf.variable_scope('branch_1'):\n",
    "            h1a = conv_bn(x,   nb_filters_2, [1,1], phase, scope='conv_1a_1x1')\n",
    "            h1b = conv_bn(h1a, nb_filters_3, [1,7], phase, scope='conv_1b_1x7')\n",
    "            h1  = conv_bn(h1b, nb_filters_4, [7,1], phase, scope='conv_1c_7x1')\n",
    "            \n",
    "        with tf.variable_scope('branch_2'):\n",
    "            h2a = conv_bn(x,   nb_filters_2, [1,1], phase, scope='conv_2a_1x1')\n",
    "            h2b = conv_bn(h2a, nb_filters_2, [1,7], phase, scope='conv_2b_1x7')\n",
    "            h2c = conv_bn(h2b, nb_filters_3, [7,1], phase, scope='conv_2c_7x1')\n",
    "            h2d = conv_bn(h2c, nb_filters_3, [1,7], phase, scope='conv_2d_1x7')\n",
    "            h2  = conv_bn(h2d, nb_filters_4, [7,1], phase, scope='conv_2e_7x1')\n",
    "            \n",
    "        return tf.concat([shortcut, h1, h2], 3, 'concat')\n",
    "        \n",
    "def inception_C():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "x         = tf.placeholder('float32', (None, 32, 32, 1), name='x')\n",
    "y_one_hot = tf.placeholder('float32', (None, nb_classes), name='y_one_hot')\n",
    "phase     = tf.placeholder(tf.bool, name='phase')\n",
    "\n",
    "h01 = conv_bn(x,   32,  [3, 3], phase, stride=1, scope='layer01')\n",
    "h02 = conv_bn(h01, 64, [3, 3], phase, stride=1, scope='layer02')\n",
    "\n",
    "h03 = residuum_stack(h02, 128,  [3, 3], phase, scope='layer03')\n",
    "h04 = inception_A(h03, phase, 'block_A')\n",
    "h05 = conv_bn(h04, 64, [3, 3], phase, stride=2, scope='layer05')\n",
    "h06 = inception_B(h05, phase, 'block_B')\n",
    "h0e = residuum_stack(h03, 128, [3, 3], phase, scope='layer0e')\n",
    "\n",
    "h00 = tf.contrib.layers.flatten(h0e, scope='flatten')\n",
    "h1  = dense_bn(h00, 96, phase, scope='layer1')\n",
    "logits = dense(h1, nb_classes, scope='logits')\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    accuracy = tf.reduce_mean(tf.cast(\n",
    "            tf.equal(tf.argmax(y_one_hot, 1), tf.argmax(logits, 1)),\n",
    "            'float32'))\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_one_hot))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(nb_epochs, X_batches, Y_batches, learning_rate):\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        # Ensures that we execute the update_ops before performing the train_step\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "\n",
    "        history = []\n",
    "        for epoch in range(0, nb_epochs):\n",
    "            X_batches, Y_batches = shuffle(X_batches, Y_batches)\n",
    "\n",
    "            for X_batch, Y_batch in zip(X_batches, Y_batches):\n",
    "                session.run(train_step,\n",
    "                             feed_dict={'x:0': X_batch, \n",
    "                                        'y_one_hot:0': Y_batch, \n",
    "                                        'phase:0': 1})\n",
    "            tr = session.run([loss, accuracy], \n",
    "                              feed_dict={'x:0': X_batch,\n",
    "                                         'y_one_hot:0': Y_batch,\n",
    "                                         'phase:0': 1})\n",
    "            t = session.run([loss, accuracy], \n",
    "                         feed_dict={'x:0': X_batch,\n",
    "                                    'y_one_hot:0': Y_batch,\n",
    "                                    'phase:0': 0})\n",
    "            history += [[epoch] + tr + t]\n",
    "            print(\"EPOCH {0} {1}\".format(epoch+1, history[-1]))\n",
    "\n",
    "#            print(history[-1])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nb_epochs = 2\n",
    "batch_size = 128\n",
    "nb_samples = 1 * batch_size\n",
    "lr = 0.1\n",
    "X_batches, Y_batches = get_batches(X['train'][0:nb_samples,::],\n",
    "                                   Y['train'][0:nb_samples,::],\n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "history = train(nb_epochs, X_batches, Y_batches, learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aa = []\n",
    "for h in history:\n",
    "    aa.append(h[1])\n",
    "plt.plot(aa)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1/43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aaa = range(1,1)\n",
    "for a in aaa:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "whos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "conv_params = dict({'kernel_size': [], 'nb_kernels':[]})\n",
    "conv_params['kernel_size'] = [[5, 5],\n",
    "                              [3, 3],\n",
    "                              [3, 1],\n",
    "                              [1, 3],\n",
    "                              [5, 5]]\n",
    "conv_params['nb_kernels'] = [6,\n",
    "                             6,\n",
    "                             12,\n",
    "                             12,\n",
    "                             32]    \n",
    "\n",
    "x         = tf.placeholder(tf.float32, shape=(None, 32, 32, 1))\n",
    "one_hot_y = tf.placeholder(tf.float32, shape=(None, nb_classes))\n",
    "\n",
    "logits = create_model(x, params=conv_params, nb_classes=nb_classes, scope_reuse=False)\n",
    "\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=one_hot_y)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.02).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "def evaluate(X_batches, Y_batches, accuracy_operation):\n",
    "    \n",
    "    total_accuracy   = 0\n",
    "    nb_total_samples = 0\n",
    "    sess = tf.get_default_session()\n",
    "    \n",
    "    for X_batch, Y_batch in zip(X_batches, Y_batches):\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: X_batch, one_hot_y: Y_batch})\n",
    "        \n",
    "        nb_samples        = len(Y_batch)\n",
    "        total_accuracy   += (accuracy * nb_samples)\n",
    "        nb_total_samples += nb_samples\n",
    "\n",
    "    return total_accuracy / nb_total_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nb_epochs = 1\n",
    "nb_samples = 300\n",
    "batch_size = 15\n",
    "losses = []\n",
    "validation_accuracy = []\n",
    "X_batches, Y_batches = get_batches(X['train'][0:nb_samples,::],\n",
    "                                   Y['train'][0:nb_samples,::],\n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "\n",
    "for X_batch, Y_batch in zip(X_batches, Y_batches):\n",
    "    assert(X_batch.shape[0]==batch_size)\n",
    "    assert(Y_batch.shape[0]==batch_size)\n",
    "    \n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "checkpoint_file = \"./checkpoints/model.ckpt\"\n",
    "checkpoint_path = \"\"\n",
    "\n",
    "with tf.Session() as session:\n",
    "    # Restore variables from disk.\n",
    "    if os.path.isfile(checkpoint_file + \".meta\"):\n",
    "        saver.restore(session, tf.train.latest_checkpoint('./checkpoints/'))\n",
    "        print(\"Model restored.\")\n",
    "    else:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "    for epoch in range(nb_epochs):\n",
    "        print(\"EPOCH {} ...\".format(epoch+1))\n",
    "        X_batches, Y_batches = shuffle(X_batches, Y_batches)\n",
    "        for X_batch, Y_batch in zip(X_batches, Y_batches):\n",
    "            assert(X_batch.shape[0]==batch_size)\n",
    "            assert(Y_batch.shape[0]==batch_size)\n",
    "            X_batch, Y_batch = shuffle(X_batch, Y_batch)\n",
    "            _,l = session.run([optimizer, loss], feed_dict={x:X_batch, one_hot_y:Y_batch})\n",
    "            losses.append(l)\n",
    "            print(\"Loss = {:.3f}\".format(l))\n",
    "\n",
    "        acc = evaluate(X_batches[0:2], Y_batches[0:2], accuracy_operation)\n",
    "        validation_accuracy.append(acc)\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(acc))\n",
    "        # Save the variables to disk.\n",
    "        checkpoint_path = saver.save(session, checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(checkpoint_file)\n",
    "print(checkpoint_path)\n",
    "\n",
    "print(os.path.exists(checkpoint_file + '*'))\n",
    "os.path.exists('./checkpoint')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "XX = dict()\n",
    "for setname in setnames:\n",
    "    XX[setname] = data[setname]['features']\n",
    "   \n",
    "\n",
    "res = normalize_brightness(XX)\n",
    "\n",
    "print(res[\"train\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDos\n",
    "* add brightness normalization\n",
    "* augement dataset to get balanced labels distribution\n",
    "* run full dataset on AWS\n",
    "* start with tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(X[\"train\"].shape[0:3].append(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.reset_default_graph()\n",
    "x         = tf.placeholder('float32', (None, 32, 32, 1), name='x')\n",
    "y_one_hot = tf.placeholder('float32', (None, nb_classes), name='y_one_hot')\n",
    "phase     = tf.placeholder(tf.bool, name='phase')\n",
    "\n",
    "h01 = conv_bn(x,  128, [3, 3], phase, scope='layer01')\n",
    "h02 = conv_bn(h01, 64, [3, 3], phase, scope='layer02')\n",
    "h03 = conv_bn(h02, 32, [3, 3], phase, scope='layer03')\n",
    "h04 = conv_bn(h03, 16, [3, 3], phase, scope='layer04')\n",
    "h05 = conv_bn(h04, 8,  [3, 3], phase, scope='layer05')\n",
    "h0e = conv_bn(h05, 4,  [3, 3], phase, scope='layer0e')\n",
    "\n",
    "h00 = tf.contrib.layers.flatten(h0e, scope='flatten')\n",
    "h1  = dense_bn(h00, 96, phase, scope='layer1')\n",
    "logits = dense(h1, nb_classes, scope='logits')\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    accuracy = tf.reduce_mean(tf.cast(\n",
    "            tf.equal(tf.argmax(y_one_hot, 1), tf.argmax(logits, 1)),\n",
    "            'float32'))\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_one_hot))\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
